use serde::{Deserialize, Serialize};
use tauri::command;
use uuid::Uuid;
use std::sync::Arc;
use tokio::sync::Mutex;

#[derive(Debug, Serialize, Deserialize)]
pub struct UploadDocumentsRequest {
    pub project_id: String,
    pub file_paths: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DocumentResponse {
    pub id: String,
    pub filename: String,
    pub file_size: u64,
    pub processing_status: String,
    pub created_at: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct UploadDocumentsResponse {
    pub successful: Vec<DocumentResponse>,
    pub failed: Vec<FailedDocumentInfo>,
    pub summary: UploadSummary,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct FailedDocumentInfo {
    pub filename: String,
    pub file_path: String,
    pub error: String,
    pub error_stage: String, // "validation" | "reading" | "processing" | "embedding" | "indexing"
}

#[derive(Debug, Serialize, Deserialize)]
pub struct UploadSummary {
    pub total: usize,
    pub successful: usize,
    pub failed: usize,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ValidateFilesRequest {
    pub file_paths: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct FileValidationInfo {
    pub path: String,
    pub filename: String,
    pub size: u64,
    pub mime_type: String,
    pub is_valid: bool,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct FileValidationError {
    pub path: String,
    pub filename: String,
    pub error: String,
    pub error_type: String, // "not_found" | "too_large" | "empty" | "unsupported_format" | "permission_denied" | "other"
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ValidateFilesResponse {
    pub valid: Vec<FileValidationInfo>,
    pub invalid: Vec<FileValidationError>,
    pub summary: ValidationSummary,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ValidationSummary {
    pub total: usize,
    pub valid_count: usize,
    pub invalid_count: usize,
    pub total_size: u64,
}

#[command]
pub async fn upload_documents(
    request: UploadDocumentsRequest,
    wrapper: tauri::State<'_, crate::app_state_wrapper::AppStateWrapper>,
) -> Result<UploadDocumentsResponse, String> {
    log::info!("ğŸ“¤ ä¸Šä¼ æ–‡æ¡£è¯·æ±‚: {:?}", request);

    // è·å–åº”ç”¨çŠ¶æ€
    let state = wrapper.get_state().await?;

    // éªŒè¯è¾“å…¥
    if request.file_paths.is_empty() {
        return Err("è‡³å°‘éœ€è¦ä¸Šä¼ ä¸€ä¸ªæ–‡æ¡£".to_string());
    }

    // è§£æé¡¹ç›® ID
    let project_id = Uuid::parse_str(&request.project_id)
        .map_err(|e| format!("æ— æ•ˆçš„é¡¹ç›®ID: {}", e))?;

    // æ£€æŸ¥é¡¹ç›®æ˜¯å¦å­˜åœ¨
    {
        let project_service = state.project_service();
        let project_service_guard = project_service.lock().await;
        if project_service_guard.get_project(project_id).is_none() {
            return Err(format!("é¡¹ç›®ä¸å­˜åœ¨: {}", project_id));
        }
    }

    // å¤„ç†æ–‡æ¡£ä¸Šä¼ 
    let document_service = state.document_service();
    let mut successful_docs = Vec::new();
    let mut failed_docs = Vec::new();
    let total_files = request.file_paths.len();

    for file_path in request.file_paths {
        log::info!("ğŸ“„ å¤„ç†æ–‡ä»¶: {}", file_path);

        match process_single_document(project_id, file_path.clone(), document_service.clone()).await {
            Ok((doc_id, filename, file_size, status, created_at)) => {
                successful_docs.push(DocumentResponse {
                    id: doc_id.to_string(),
                    filename: filename.clone(),
                    file_size,
                    processing_status: status,
                    created_at: created_at.to_rfc3339(),
                });
                log::info!("âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {} (ID: {})", filename, doc_id);
            }
            Err(e) => {
                // æå–æ–‡ä»¶å
                let filename = std::path::Path::new(&file_path)
                    .file_name()
                    .and_then(|n| n.to_str())
                    .unwrap_or("æœªçŸ¥æ–‡ä»¶")
                    .to_string();

                // è§£æé”™è¯¯é˜¶æ®µ
                let (error_stage, error_message) = parse_error_stage(&e);

                failed_docs.push(FailedDocumentInfo {
                    filename: filename.clone(),
                    file_path: file_path.clone(),
                    error: error_message,
                    error_stage,
                });
                log::error!("âŒ æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {} - {}", filename, e);
            }
        }
    }

    // æ›´æ–°é¡¹ç›®çš„æ–‡æ¡£æ•°é‡
    {
        // å…ˆè®¡ç®—æ–‡æ¡£æ•°é‡ï¼ˆä»æ•°æ®åº“æŸ¥è¯¢ï¼Œç¡®ä¿æ˜¯ç´¯åŠ çš„æ€»æ•°ï¼‰
        let doc_count = {
            let doc_service = state.document_service();
            let doc_service_guard = doc_service.lock().await;
            doc_service_guard.count_documents(Some(project_id)).await
        };

        log::info!("ğŸ“Š é¡¹ç›® {} çš„æ–‡æ¡£æ€»æ•°: {}", project_id, doc_count);

        // ç„¶åæ›´æ–°é¡¹ç›®
        let project_service = state.project_service();
        let mut project_service_guard = project_service.lock().await;
        if let Some(project) = project_service_guard.get_project_mut(project_id) {
            project.document_count = doc_count as u32;
            project.updated_at = chrono::Utc::now();

            // ä¿å­˜æ›´æ–°åˆ°æ•°æ®åº“
            let project_clone = project.clone();
            let _ = project_service_guard.save_project_to_db(&project_clone);
        }
    }

    let summary = UploadSummary {
        total: total_files,
        successful: successful_docs.len(),
        failed: failed_docs.len(),
    };

    log::info!(
        "ğŸ¯ æ–‡æ¡£ä¸Šä¼ å®Œæˆ - æ€»æ•°: {}, æˆåŠŸ: {}, å¤±è´¥: {}",
        summary.total,
        summary.successful,
        summary.failed
    );

    // å³ä½¿éƒ¨åˆ†å¤±è´¥ä¹Ÿè¿”å›æˆåŠŸï¼Œè®©å‰ç«¯å¤„ç†å¤±è´¥åˆ—è¡¨
    Ok(UploadDocumentsResponse {
        successful: successful_docs,
        failed: failed_docs,
        summary,
    })
}

/// è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–é”™è¯¯é˜¶æ®µå’Œæ¸…æ™°çš„é”™è¯¯æ¶ˆæ¯
fn parse_error_stage(error: &str) -> (String, String) {
    if error.contains("[é˜¶æ®µ1-éªŒè¯]") || error.contains("æ–‡ä»¶ä¸å­˜åœ¨") {
        ("validation".to_string(), extract_error_message(error))
    } else if error.contains("[é˜¶æ®µ2-å…ƒæ•°æ®]") || error.contains("æ— æ³•è¯»å–æ–‡ä»¶ä¿¡æ¯") {
        ("reading".to_string(), extract_error_message(error))
    } else if error.contains("[é˜¶æ®µ3-è¯»å–]") || error.contains("æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹") {
        ("reading".to_string(), extract_error_message(error))
    } else if error.contains("[é˜¶æ®µ4-å¤„ç†]") || error.contains("æ–‡æ¡£å¤„ç†å¤±è´¥") {
        ("processing".to_string(), extract_error_message(error))
    } else if error.contains("embedding") || error.contains("å‘é‡") {
        ("embedding".to_string(), extract_error_message(error))
    } else if error.contains("[é˜¶æ®µ5-æŸ¥è¯¢]") || error.contains("ç´¢å¼•") {
        ("indexing".to_string(), extract_error_message(error))
    } else {
        ("unknown".to_string(), error.to_string())
    }
}

/// æå–é”™è¯¯æ¶ˆæ¯çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œå»é™¤é˜¶æ®µæ ‡è®°
fn extract_error_message(error: &str) -> String {
    // ç§»é™¤é˜¶æ®µæ ‡è®°ï¼Œåªä¿ç•™å®é™…é”™è¯¯ä¿¡æ¯
    if let Some(pos) = error.find("] ") {
        error[pos + 2..].to_string()
    } else {
        error.to_string()
    }
}

/// å¤„ç†å•ä¸ªæ–‡æ¡£çš„ä¸Šä¼ å’Œå¤„ç†
async fn process_single_document(
    project_id: Uuid,
    file_path: String,
    document_service: Arc<Mutex<crate::services::document_service::DocumentService>>,
) -> Result<(Uuid, String, u64, String, chrono::DateTime<chrono::Utc>), String> {
    use std::path::Path;
    use sha2::{Sha256, Digest};

    log::info!("ğŸ“„ [é˜¶æ®µ1/5] å¼€å§‹å¤„ç†æ–‡æ¡£: {}", file_path);

    // é˜¶æ®µ1: éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§
    let path = Path::new(&file_path);
    if !path.exists() {
        let error = format!("[é˜¶æ®µ1-éªŒè¯] æ–‡ä»¶ä¸å­˜åœ¨: {}", file_path);
        log::error!("âŒ {}", error);
        return Err(error);
    }

    // é˜¶æ®µ2: è¯»å–æ–‡ä»¶å…ƒæ•°æ®
    log::debug!("ğŸ“‹ [é˜¶æ®µ2/5] è¯»å–æ–‡ä»¶å…ƒæ•°æ®...");
    let metadata = std::fs::metadata(&file_path)
        .map_err(|e| {
            let error = format!("[é˜¶æ®µ2-å…ƒæ•°æ®] æ— æ³•è¯»å–æ–‡ä»¶ä¿¡æ¯: {} - {}", file_path, e);
            log::error!("âŒ {}", error);
            error
        })?;

    let file_size = metadata.len();

    // è·å–æ–‡ä»¶å
    let filename = path
        .file_name()
        .and_then(|name| name.to_str())
        .ok_or_else(|| {
            let error = format!("[é˜¶æ®µ2-å…ƒæ•°æ®] æ— æ•ˆçš„æ–‡ä»¶å: {}", file_path);
            log::error!("âŒ {}", error);
            error
        })?
        .to_string();

    log::info!("âœ… æ–‡ä»¶ä¿¡æ¯ - åç§°: {}, å¤§å°: {} bytes", filename, file_size);

    // é˜¶æ®µ3: è¯»å–æ–‡ä»¶å†…å®¹å¹¶è®¡ç®—å“ˆå¸Œ
    log::debug!("ğŸ” [é˜¶æ®µ3/5] è¯»å–æ–‡ä»¶å†…å®¹å¹¶è®¡ç®—å“ˆå¸Œ...");
    let content = std::fs::read(&file_path)
        .map_err(|e| {
            let error = format!("[é˜¶æ®µ3-è¯»å–] æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹: {} - {}", filename, e);
            log::error!("âŒ {}", error);
            error
        })?;

    let mut hasher = Sha256::new();
    hasher.update(&content);
    let hash = format!("{:x}", hasher.finalize());

    log::debug!("âœ… æ–‡ä»¶å“ˆå¸Œ: {}", hash);

    // é˜¶æ®µ4: æ·»åŠ æ–‡æ¡£åˆ°æœåŠ¡ï¼ˆåŒ…å«æ–‡æœ¬æå–ã€åˆ†å—ã€å‘é‡åŒ–ï¼‰
    log::info!("ğŸ“ [é˜¶æ®µ4/5] å¤„ç†æ–‡æ¡£å†…å®¹ï¼ˆæå–æ–‡æœ¬ã€åˆ†å—ã€å‘é‡åŒ–ï¼‰...");
    let mut doc_service = document_service.lock().await;
    let document_id = doc_service
        .add_document(project_id, file_path.clone(), file_size, hash)
        .await
        .map_err(|e| {
            let error_msg = e.to_string();

            // æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            let detailed_error = if error_msg.contains("Failed to extract") {
                format!("[é˜¶æ®µ4-æ–‡æœ¬æå–] æ— æ³•æå–æ–‡æ¡£å†…å®¹: {} - å¯èƒ½æ˜¯æ–‡ä»¶æŸåæˆ–æ ¼å¼ä¸æ­£ç¡®", filename)
            } else if error_msg.contains("No valid chunks") {
                format!("[é˜¶æ®µ4-åˆ†å—] æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•åˆ†å—: {} - æ–‡æ¡£å¯èƒ½æ²¡æœ‰å¯æå–çš„æ–‡æœ¬å†…å®¹", filename)
            } else if error_msg.contains("embedding") || error_msg.contains("API") {
                format!("[é˜¶æ®µ4-å‘é‡åŒ–] å‘é‡åŒ–å¤±è´¥: {} - APIè°ƒç”¨é”™è¯¯æˆ–ç½‘ç»œé—®é¢˜", filename)
            } else if error_msg.contains("Unsupported file type") {
                format!("[é˜¶æ®µ4-æ ¼å¼] ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {} - {}", filename, error_msg)
            } else {
                format!("[é˜¶æ®µ4-å¤„ç†] æ–‡æ¡£å¤„ç†å¤±è´¥: {} - {}", filename, error_msg)
            };

            log::error!("âŒ {}", detailed_error);
            detailed_error
        })?;

    log::info!("âœ… æ–‡æ¡£å¤„ç†æˆåŠŸï¼ŒID: {}", document_id);

    // é˜¶æ®µ5: è·å–æ–‡æ¡£ä¿¡æ¯
    log::debug!("ğŸ“Š [é˜¶æ®µ5/5] è·å–æ–‡æ¡£çŠ¶æ€...");
    let document = doc_service
        .get_document(document_id)
        .ok_or_else(|| {
            let error = format!("[é˜¶æ®µ5-æŸ¥è¯¢] æ–‡æ¡£æ·»åŠ åæœªæ‰¾åˆ°: {}", filename);
            log::error!("âŒ {}", error);
            error
        })?;

    log::info!(
        "ğŸ‰ æ–‡æ¡£å¤„ç†å®Œæˆ: {} (çŠ¶æ€: {}, chunks: {})",
        filename,
        document.processing_status,
        document.chunk_count
    );

    Ok((
        document.id,
        document.filename.clone(),
        document.file_size,
        document.processing_status.to_string(),
        document.created_at,
    ))
}

/// æ‰¹é‡éªŒè¯æ–‡ä»¶
/// åœ¨å®é™…å¤„ç†å‰è¿›è¡Œé¢„æ£€æŸ¥ï¼Œå¿«é€Ÿè¯†åˆ«æ— æ•ˆæ–‡ä»¶
#[command]
pub async fn validate_files(
    request: ValidateFilesRequest,
) -> Result<ValidateFilesResponse, String> {
    log::info!("æ‰¹é‡éªŒè¯æ–‡ä»¶è¯·æ±‚: {} ä¸ªæ–‡ä»¶", request.file_paths.len());

    let mut valid = Vec::new();
    let mut invalid = Vec::new();
    let mut total_size: u64 = 0;

    for file_path in request.file_paths {
        match validate_single_file(&file_path).await {
            Ok(info) => {
                total_size += info.size;
                valid.push(info);
            }
            Err(error_info) => {
                invalid.push(error_info);
            }
        }
    }

    let summary = ValidationSummary {
        total: valid.len() + invalid.len(),
        valid_count: valid.len(),
        invalid_count: invalid.len(),
        total_size,
    };

    log::info!(
        "æ–‡ä»¶éªŒè¯å®Œæˆ - æ€»æ•°: {}, æœ‰æ•ˆ: {}, æ— æ•ˆ: {}, æ€»å¤§å°: {} MB",
        summary.total,
        summary.valid_count,
        summary.invalid_count,
        total_size / (1024 * 1024)
    );

    Ok(ValidateFilesResponse {
        valid,
        invalid,
        summary,
    })
}

/// éªŒè¯å•ä¸ªæ–‡ä»¶
async fn validate_single_file(
    file_path: &str,
) -> Result<FileValidationInfo, FileValidationError> {
    use std::path::Path;

    let path = Path::new(file_path);

    // è·å–æ–‡ä»¶å
    let filename = path
        .file_name()
        .and_then(|name| name.to_str())
        .unwrap_or("unknown")
        .to_string();

    // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if !path.exists() {
        return Err(FileValidationError {
            path: file_path.to_string(),
            filename,
            error: "æ–‡ä»¶ä¸å­˜åœ¨".to_string(),
            error_type: "not_found".to_string(),
        });
    }

    // æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶
    if !path.is_file() {
        return Err(FileValidationError {
            path: file_path.to_string(),
            filename,
            error: "è·¯å¾„ä¸æ˜¯æ–‡ä»¶".to_string(),
            error_type: "not_found".to_string(),
        });
    }

    // è·å–æ–‡ä»¶å…ƒæ•°æ®
    let metadata = match std::fs::metadata(path) {
        Ok(m) => m,
        Err(e) => {
            let error_type = if e.kind() == std::io::ErrorKind::PermissionDenied {
                "permission_denied"
            } else {
                "other"
            };
            return Err(FileValidationError {
                path: file_path.to_string(),
                filename,
                error: format!("æ— æ³•è¯»å–æ–‡ä»¶ä¿¡æ¯: {}", e),
                error_type: error_type.to_string(),
            });
        }
    };

    let file_size = metadata.len();

    // æ£€æŸ¥æ–‡ä»¶å¤§å°
    const MAX_FILE_SIZE: u64 = 50 * 1024 * 1024; // 50MB
    if file_size > MAX_FILE_SIZE {
        return Err(FileValidationError {
            path: file_path.to_string(),
            filename,
            error: format!(
                "æ–‡ä»¶è¿‡å¤§: {:.2} MB (æœ€å¤§: 50 MB)",
                file_size as f64 / (1024.0 * 1024.0)
            ),
            error_type: "too_large".to_string(),
        });
    }

    // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
    if file_size == 0 {
        return Err(FileValidationError {
            path: file_path.to_string(),
            filename,
            error: "æ–‡ä»¶ä¸ºç©º".to_string(),
            error_type: "empty".to_string(),
        });
    }

    // æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ
    let extension = path
        .extension()
        .and_then(|ext| ext.to_str())
        .unwrap_or("");

    let supported_extensions = vec!["txt", "md", "markdown", "pdf", "doc", "docx", "rtf"];
    if !supported_extensions.contains(&extension.to_lowercase().as_str()) {
        return Err(FileValidationError {
            path: file_path.to_string(),
            filename,
            error: format!(
                "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .{} (æ”¯æŒ: {})",
                extension,
                supported_extensions.join(", ")
            ),
            error_type: "unsupported_format".to_string(),
        });
    }

    // æ£€æµ‹ MIME ç±»å‹
    let mime_type = match extension.to_lowercase().as_str() {
        "txt" => "text/plain",
        "md" | "markdown" => "text/markdown",
        "pdf" => "application/pdf",
        "doc" => "application/msword",
        "docx" => "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        "rtf" => "application/rtf",
        _ => "application/octet-stream",
    };

    Ok(FileValidationInfo {
        path: file_path.to_string(),
        filename,
        size: file_size,
        mime_type: mime_type.to_string(),
        is_valid: true,
    })
}

#[command]
pub async fn get_document_content(_document_id: String) -> Result<String, String> {
    // TODO: Implement get document content
    Err("Not implemented".to_string())
}
